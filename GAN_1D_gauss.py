import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# seed = 21
# np.random.seed(seed)
# tf.set_random_seed(seed)

def gauss(x, mu, sigma):
    return (1/np.sqrt(2*3.1415*sigma*sigma)) * np.exp(-(x-mu)*(x-mu)/(2*sigma*sigma))

d_input = 1
d_hidden_1 = 3
d_hidden_2 = 3
d_hidden_3 = 3
d_out = 1

def createGNN(x, weights, biases):
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['h1'])
    layer_1 = tf.nn.softplus(layer_1)

    # layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['h2'])
    # layer_2 = tf.nn.softplus(layer_2)

    layer_out = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])
    # layer_out = tf.nn.sigmoid(layer_out)

    params = [weights['h1'], biases['h1'], weights['out'], biases['out']]

    return layer_out, params

def createDNN(x, weights, biases):
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['h1'])
    layer_1 = tf.nn.tanh(layer_1)

    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['h2'])
    layer_2 = tf.nn.tanh(layer_2)

    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['h3'])
    layer_3 = tf.nn.tanh(layer_3)

    layer_out = tf.add(tf.matmul(layer_3, weights['out']), biases['out'])
    layer_out = tf.nn.sigmoid(layer_out)

    params = [weights['h1'], biases['h1'], weights['h2'], biases['h2'], weights['h3'], biases['h3'], weights['out'], biases['out']]

    return layer_out, params

stdD = 1.0
stdG = 1.0
weightsD = {
    'h1': tf.Variable(tf.random_normal([d_input, d_hidden_1], mean=0.0, stddev=stdD)),
    'h2': tf.Variable(tf.random_normal([d_hidden_1, d_hidden_2], mean=0.0, stddev=stdD)),
    'h3': tf.Variable(tf.random_normal([d_hidden_2, d_hidden_3], mean=0.0, stddev=stdD)),
    'out': tf.Variable(tf.random_normal([d_hidden_3, d_out], mean=0.0, stddev=stdD))
}
biasesD = {
    'h1': tf.Variable(tf.random_normal([d_hidden_1], mean=0.0, stddev=stdD)),
    'h2': tf.Variable(tf.random_normal([d_hidden_2], mean=0.0, stddev=stdD)),
    'h3': tf.Variable(tf.random_normal([d_hidden_3], mean=0.0, stddev=stdD)),
    'out': tf.Variable(tf.random_normal([d_out], mean=0.0, stddev=stdD))
}

weightsG = {
    'h1': tf.Variable(tf.random_normal([d_input, d_hidden_1], mean=0.0, stddev=stdG)),
    'h2': tf.Variable(tf.random_normal([d_hidden_1, d_hidden_2], mean=0.0, stddev=stdG)),
    'h3': tf.Variable(tf.random_normal([d_hidden_2, d_hidden_3], mean=0.0, stddev=stdG)),
    'out': tf.Variable(tf.random_normal([d_hidden_3, d_out], mean=0.0, stddev=stdG))
}
biasesG = {
    'h1': tf.Variable(tf.random_normal([d_hidden_1], mean=0.0, stddev=stdG)),
    'h2': tf.Variable(tf.random_normal([d_hidden_2], mean=0.0, stddev=stdG)),
    'h3': tf.Variable(tf.random_normal([d_hidden_3], mean=0.0, stddev=stdG)),
    'out': tf.Variable(tf.random_normal([d_out], mean=0.0, stddev=stdG))
}

with tf.variable_scope('G') as scope:
    g_x = tf.placeholder("float", [None, d_input])
    G, g_params = createGNN(g_x, weightsG, biasesG)

with tf.variable_scope('D') as scope:
    d_x = tf.placeholder("float", [None, d_input])
    D1, d1_params = createDNN(d_x, weightsD, biasesD)
    scope.reuse_variables()
    D2, d2_params = createDNN(G, weightsD, biasesD)

# value function for descrimintator:
#  log(D1(x)) + log(1 - D2(G(z)))
#  where:
#  D1 = discriminator function on real inputs
#  D2 = discriminator function on false inputs
#   NOTE: D1 and D2 are copies of descriminator function F
#  G = generator function
#  x = sample from real data distribution
#  G(z) = x' = fake data generated by G

def optimizer(loss, var_list):
    initial_learning_rate = 0.03
    decay = 0.95
    num_decay_steps = 150
    batch = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(initial_learning_rate, batch, num_decay_steps, decay, staircase=True)
    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.6).minimize(loss, global_step=batch, var_list=var_list)
    return optimizer

# training the descriminator (D) to minimise D2 and maximise D1
# learning_rate_d = 0.05
batch = tf.Variable(0)
obj_d = tf.reduce_mean(-tf.log(D1) - tf.log(1 - D2))
# opt_d = tf.train.GradientDescentOptimizer(learning_rate_d).minimize(1-obj_d, global_step=batch)#, var_list=theta_d)
opt_d = optimizer(obj_d, d1_params)

# training the generator (G) to maximise D2
#  log(D2(G(z)))2
# learning_rate_g = 0.05
batch = tf.Variable(0)
obj_g = tf.reduce_mean(-tf.log(D2))
# g_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope="D")
# opt_g = tf.train.GradientDescentOptimizer(learning_rate_g).minimize(1-obj_g, global_step=batch)#, var_list=g_params)
opt_g = optimizer(obj_g, g_params)

# pre-training of the discriminator
preY = tf.placeholder("float", [None, d_input])
obj_pre_d = tf.reduce_mean(tf.square(D1 - preY))
# opt_pre_d = tf.train.GradientDescentOptimizer(learning_rate_d).minimize(obj_pre_d, global_step=batch)#, var_list=theta_d)
opt_pre_d = optimizer(obj_pre_d, None)

#initialise the variables
init = tf.global_variables_initializer()
epochs = 2200
epochStartGen = 1000
M = 100
mu = 1
sigma = 0.5
discCount = 1

# graph the outputs
def graph():
    # come sample data
    sampleBatch = 10000
    x_v = np.linspace(-5, 5, sampleBatch)
    bins = np.linspace(-5, 5, 100)

    # sample the generator
    y_g_v = np.reshape(G.eval({g_x : np.reshape(x_v, (sampleBatch, 1))}), (sampleBatch, 1))
    y_g_v, _ = np.histogram(y_g_v, bins=bins, density=True)

    # sample the discriminator
    y_d_v = np.reshape(D1.eval({d_x : np.reshape(x_v, (sampleBatch, 1))}), (sampleBatch, 1))
    # y_d_v, _ = np.histogram(y_d_v, bins=bins, density=True)


    # sample the data distribution
    y_data = np.random.normal(mu, sigma, sampleBatch) # gauss(x_v, mu, sigma)#
    y_data, _ = np.histogram(y_data, bins=bins, density=True)

    x_v_plot = np.linspace(-5, 5, len(y_data))

    # plot the results
    plt.figure(1)
    plt.title("GAN OUTPUT")
    plt.plot(x_v_plot, y_g_v, label="generator")
    plt.plot(x_v, y_d_v, label="discriminator")
    plt.plot(x_v_plot, y_data, label="data")
    plt.legend()
    plt.show()

#launch the graph
with tf.Session() as sess:
    sess.run(init)

    # we now train both networks:
    for i in range(epochs):
        if i < epochStartGen:
            x = (np.random.random(M) - 0.5) * 10.0
            labels = gauss(x, mu, sigma)
            sess.run(opt_pre_d, {d_x: np.reshape(x, (M, 1)), preY: np.reshape(labels, (M, 1))})

            print("epoch pretrain: {} complete".format(i))
        elif i == epochStartGen:
            graph()
        else:
            for j in range(discCount):
                # sample minibatch from REAL data
                x = (np.random.normal(mu, sigma, M))
                x.sort()
                # create some random data to generate FAKE x'
                z = np.linspace(-5., 5., M) + np.random.random(M) * 0.01 #stratified
                # run optimisation on the descriminator network
                sess.run(opt_d, {d_x: np.reshape(x, (M, 1)), g_x: np.reshape(z, (M, 1))})
                d_loss = sess.run(obj_d, {d_x: np.reshape(x, (M, 1)), g_x: np.reshape(z, (M, 1))})

            # create some more random data to generate FAKE x'
            z = np.linspace(-5., 5., M) + np.random.random(M) * 0.01 #stratified
            # run optimisation on the generative network
            sess.run(opt_g, {g_x: np.reshape(z, (M, 1))})
            g_loss = sess.run(obj_g, {g_x: np.reshape(z, (M, 1))})

            # y = np.reshape(G.eval({g_x : np.reshape(z, (M, 1))}), (M, 1))
            if i % 100 == 0:
                print("epoch GAN: {} complete".format(i - epochStartGen))
                print("G loss: {}".format(g_loss))
                print("D loss: {}".format(d_loss))

            if i % 100 == 0:
                graph()

    graph()

    sess.close()



